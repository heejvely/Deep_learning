{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"밑바닥부터 만들어보는 딥러닝.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSswfLehw42r1/QDZRnnR5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from numpy import ndarray\n","from typing import *"],"metadata":{"id":"gjUFX977Tp4f","executionInfo":{"status":"ok","timestamp":1642504818983,"user_tz":-540,"elapsed":301,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def assert_same_shape(array: ndarray,\n","                      array_grad: ndarray):\n","  assert array.shape == array_grad.shape, \\\n","  f\"\"\"\n","  두 ndarray의 모양이 같아야 하는데,\n","  첫 번째 ndarray의 모양은 {tuple(array_grad.shape)}이고,\n","  두 번째 ndarray의 모양은 {typle(array.shape)}이다.\n","  \"\"\"\n","  return None"],"metadata":{"id":"iIP2LetnTwOV","executionInfo":{"status":"ok","timestamp":1642504819761,"user_tz":-540,"elapsed":502,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### 신경망 구성 요소: 연산"],"metadata":{"id":"nfrxY1JpSqAi"}},{"cell_type":"markdown","source":["Operation 클래스"],"metadata":{"id":"N73CjkPSTnPL"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"ppn7xvr4QFUc","executionInfo":{"status":"ok","timestamp":1642504819762,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"outputs":[],"source":["class Operation(object):\n","  \"\"\"\n","  신경망 모델의 연산 역할을 하는 기반 클래스\n","  \"\"\"\n","  def __init__(self):\n","    pass\n","\n","  def forward(self, input_: ndarray):\n","    \"\"\"\n","    인스턴스 변수 self._input에 입력값을 저장한 다음 self._output() 함수를 호출한다.\n","    \"\"\"\n","    self.input_ = input_\n","    self.output = self._output()\n","    return self.output\n","\n","  def backward(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    self._input_grad() 함수를 호출한다. 이때 모양이 일치하는지 먼저 확인한다.\n","    \"\"\"\n","    assert_same_shape(self.output, output_grad)\n","    self.input_grad = self._input_grad(output_grad)\n","\n","    assert_same_shape(self.input_, self.input_grad)\n","    return self.input_grad\n","\n","  def _output(self) -> ndarray:\n","    \"\"\"\n","    Operation을 구현한 모든 구상 클래서는 _output 메서드를 구현해야 한다.\n","    \"\"\"\n","    raise NotImplementedError()\n","\n","  def _input_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    Operation을 구현한 모든 구상 클래스는 _input_grad 메서드를 구현해야 한다.\n","    \"\"\"\n","    raise NotImplementedError()"]},{"cell_type":"markdown","source":["PraramOperation 클래스"],"metadata":{"id":"XBQuHTNFUstj"}},{"cell_type":"code","source":["class ParamOperation(Operation):\n","  \"\"\"\n","  파라미터를 갖는 연산\n","  \"\"\"\n","\n","  def __init__(self, param: ndarray) -> ndarray:\n","    \"\"\"\n","    생성자 메서트\n","    \"\"\"\n","    super().__init__()\n","    self.param = param\n","\n","  def backward(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    self._input_grad, self._param_grad를 호출한다.\n","    이때 ndarray 객체의 모양이 일치하는지 확인한다.\n","    \"\"\"\n","    \n","    assert_same_shape(self.output, output_grad)\n","\n","    self.input_grad = self._input_grad(output_grad)\n","    self.param_grad = self._param_grad(output_grad)\n","\n","    assert_same_shape(self.input_, self.input_grad)\n","    assert_same_shape(self.param, self.param_grad)\n","\n","    return self.input_grad\n","\n","  def _param_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    ParamOperation을 구현한 모든 구상 클래스는 _param_grad 메서드를 구현해야 한다.\n","    \"\"\"\n","    raise NotImplementedError()"],"metadata":{"id":"ayR4lMbNUeAL","executionInfo":{"status":"ok","timestamp":1642504819762,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#### 신경망의 구성 요소: 층"],"metadata":{"id":"9po8u8goVwZN"}},{"cell_type":"markdown","source":["Operaion의 구상 클래스"],"metadata":{"id":"RDRU7aEUXiYF"}},{"cell_type":"markdown","source":["WeightMulitiply 클래스( 파라미터 행렬과 입력 행렬의 행렬곱 연산)"],"metadata":{"id":"7U_NlsmEXoos"}},{"cell_type":"code","source":["class WeightMultiply(ParamOperation):\n","  \"\"\"\n","  신경망의 가중치 행렬곱 연산\n","  \"\"\"\n","\n","  def __init__(self, W: ndarray):\n","    \"\"\"\n","    self.param = W로 초기화\n","    \"\"\"\n","    super().__init__(W)\n","\n","  def _output(self) -> ndarray:\n","    \"\"\"\n","    출력값 계산\n","    \"\"\"\n","    return np.dot(self.input_, self.param)\n","\n","  def _input_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    입력에 대한 기울기 계산\n","    \"\"\"\n","    return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n","\n","  def _param_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    파라미터에 대한 기울기 계산\n","    \"\"\"\n","    return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"],"metadata":{"id":"kkwMstcKVsUt","executionInfo":{"status":"ok","timestamp":1642504819762,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["BiadAdd 클래스(편향 향을 더하는 덧셈 연산)"],"metadata":{"id":"VM018-03Yd3W"}},{"cell_type":"code","source":["class BiasAdd(ParamOperation):\n","  \"\"\"\n","  편향을 더하는 연산\n","  \"\"\"\n","\n","  def __init__(self, B: ndarray):\n","    \"\"\"\n","    self.param = B로 초기화한다.\n","    초기화 전에 행렬의 모양을 확인한다.\n","    \"\"\"\n","    assert B.shape[0] == 1\n","\n","    super().__init__(B)\n","\n","  def _output(self) -> ndarray:\n","    \"\"\"\n","    출력값 계산\n","    \"\"\"\n","    return self.input_ + self.param\n","\n","  def _input_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    입력에 대한 기울기 계산\n","    \"\"\"\n","    return np.ones_like(self.input_) * output_grad\n","\n","  def _param_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    파라미터에 대한 기울기 계산\n","    \"\"\"\n","    param_grad = np.ones_like(self.param) * output_grad\n","    return np.sum(param_grad, axis = 0).reshape(1, param_grad.shape[1])"],"metadata":{"id":"jVg7fNBvYX3T","executionInfo":{"status":"ok","timestamp":1642504819762,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Sigmoid 클래스(sigmoid 활성화 함수)"],"metadata":{"id":"nooUQELBZVKx"}},{"cell_type":"code","source":["class Sigmoid(Operation):\n","  \"\"\"\n","  Sigmoid 활성화 함수\n","  \"\"\"\n","\n","  def __init__(self) -> None:\n","    \"\"\" Pass\"\"\"\n","    super().__init__()\n","\n","  def _output(self) -> ndarray:\n","    \"\"\"\n","    출력값 계산\n","    \"\"\"\n","    return 1.0 / (1.0 + np.exp(-1.0 * self.input_))\n","\n","  def _input_grad(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    입력에 대한 기울기 계산\n","    \"\"\"\n","    sigmoid_backward = self.output * (1.0 - self.output)\n","    input_grad = sigmoid_backward * output_grad\n","    return input_grad"],"metadata":{"id":"Guk4QdBDZUg3","executionInfo":{"status":"ok","timestamp":1642504819763,"user_tz":-540,"elapsed":6,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Linear 클래스"],"metadata":{"id":"U1zp8GGqcbZl"}},{"cell_type":"code","source":["class Linear(Operation):\n","  \"\"\"\n","  항등 활성화 함수\n","  \"\"\"\n","  def __init__(self) -> None:\n","    '기반 클래스의 생성자 메서드 실행'\n","    super().__init__()\n","\n","  def _output(self) -> ndarray:\n","    '입력을 그대로 출력'\n","    return self.input_\n","  \n","  def _input_grad(self, output_grad: ndarray) -> ndarray:\n","    '그대로 출력'\n","    return output_grad"],"metadata":{"id":"hZr7XrHPZ8S7","executionInfo":{"status":"ok","timestamp":1642504819763,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#### Layer 클래스 설계\n","\n","- forward와 backward, 이 2개의 메서드는 층에 포함된 일련의 Operation 클래스 객체에 입력값을 순서대로 통과시키는 역할을 한다. 이 과정은 지금까지 우리가 다이어그램을 그려보며 해온 것과 완전히 같다. 이 부분이 실질적으로 Layer 객체가 하는 일이고, 나머지 코드는 코드를 감싸는 래퍼(wrapper) 역할이나 정보를 저장하는 역할을 맞는다.\n","  - _setup_layer 메서드를 통해 정확한 순서대로 Operation 클래스 객체의 연속을 정의하고 각 Operation 객체의 파라미터를 저장해서 초기화한다.\n","  - forward 메서드는 self.input_과 self.output에 각각 값을 저장한다.\n","  - backward 메더스는 역방향 계산을 수행하기 전, 행렬의 모양을 먼저 검사한다.\n","- 마지막으로 _param._param_grads 메서드는 층에 포함된 ParamOperation 클래스 객체에서 파라미터와 파라미터에 대한 기울기의 값을 꺼낸다."],"metadata":{"id":"1jwEtxwDc0Hv"}},{"cell_type":"code","source":["class Layer(object):\n","  \"\"\"\n","  신경망 모델의 층 역할을 하는 클래스\n","  \"\"\"\n","\n","  def __init__(self, neurons: int):\n","    \"\"\"\n","    뉴런의 개수는 층의 너비에 해당한다.\n","    \"\"\"\n","    self.neurons = neurons\n","    self.first = True\n","    self.params: List[ndarray] = []\n","    self.param_grads: List[ndarray] = []\n","    self.operations: List[Operation] = []\n","\n","  def _setup_layer(self, num_in: int) -> None:\n","    \"\"\"\n","    Layer를 구현하는 구상 클래스는 _setup_layer 메서드를 구현해야 한다.\n","    \"\"\"\n","    raise NotImplementedError()\n","\n","  def forward(self, input_: ndarray) -> ndarray:\n","    \"\"\"\n","    입력값을 각 연산에 순서대로 통과시켜 순방향 계산을 수행한다.\n","    \"\"\"\n","    if self.first:\n","      self._setup_layer(input_)\n","      self.first = False\n","\n","    self.input_ = input_\n","\n","    for operation in self.operations:\n","      input_ = operation.forward(input_)\n","\n","    self.output = input_\n","\n","    return self.output\n","\n","  def backward(self, output_grad: ndarray) -> ndarray:\n","    \"\"\"\n","    output_grad를 각 연산에 역순으로 통과시켜 역방향 계산을 수행한다.\n","    계산하기 전, 행렬의 모양을 검사한다.\n","    \"\"\"\n","\n","    assert_same_shape(self.output, output_grad)\n","\n","    for operation in reversed(self.operations):\n","      output_grad = operation.backward(output_grad)\n","\n","    input_grad = output_grad\n","\n","    self._param_grads()\n","\n","    return input_grad\n","\n","  def _param_grads(self) -> ndarray:\n","    \"\"\"\n","    각 Operation 객체에서 _param_grad 값을 꺼낸다.\n","    \"\"\"\n","\n","    self.param_grads = []\n","    for operation in self.operations:\n","      if issubclass(operation.__class__, ParamOperation):      # 상속관계여부 확인 issubclass(자식 클래스, 부모 클래스)\n","        self.param_grads.append(operation.param_grad)\n","\n","  def _params(self) -> ndarray:\n","    \"\"\"\n","    각 Operation 객체에서 _params값을 꺼낸다.\n","    \"\"\"\n","\n","    self.params = []\n","    for operation in self.operations:\n","      if issubclass(operation.__class__, ParamOperation):\n","        self.params.append((operation.param))"],"metadata":{"id":"zCREGqZXczCm","executionInfo":{"status":"ok","timestamp":1642504819763,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### 밀집층 구현하기"],"metadata":{"id":"lBV0ZEdegPFd"}},{"cell_type":"code","source":["class Dense(Layer):\n","  \"\"\"\n","  Layer 클래스를 구현한 전결합층\n","  \"\"\"\n","  def __init__(self,\n","               neurons: int,\n","               activation: Operation = Sigmoid()) -> None:\n","    \"\"\"\n","    초기화 시 활성화 함수를 결정해야 함\n","    \"\"\"\n","    super().__init__(neurons)\n","    self.activation = activation\n","\n","  def _setup_layer(self, input_: ndarray) -> None:\n","    \"\"\"\n","    전결합층의 연산을 정의\n","    \"\"\"\n","    if self.seed:\n","      np.random.seed(self.seed)\n","    \n","    self.params = []\n","\n","    # 가중치\n","    self.params.append(np.random.randn(input_.shape[1], self.neurons))\n","\n","    # 편향\n","    self.params.append(np.random.randn(1, self.neurons))\n","\n","    self.operations = [WeightMultiply(self.params[0]),\n","                       BiasAdd(self.params[1]),\n","                       self.activation]\n","\n","    return None"],"metadata":{"id":"2i3QFMPIgKPk","executionInfo":{"status":"ok","timestamp":1642504819763,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## NeuralNetwork 클래스와 그 외 클래스\n","- 데이터로부터 학습하는 클래스\n","\n","1. 신경망은 X를 입력받아 각 Layer(여러개의 Operation을 묶은 편리한 래퍼)를 순섣로 통과시킨다. 신경망 끝에서 출력된 값이 예측값 prediction이 된다.\n","2. prediction을 y와 비교하여 손실을 계산한 다음, 이 손실로 마지막 층(prediction을 출력한 층)의 각 요소에 대한 '손실의 기울기'를 계산한다.\n","3. 마지막으로 손실의 기울기를 각 층에 역순으로 전달한다. 그 과정에서 각 Operation 객체마다 '파라미터에 대한 손실의 기울기'를 계산하고 해당 객체에 그 값을 저장한다."],"metadata":{"id":"i5Y_nzfvlKsQ"}},{"cell_type":"markdown","source":["Loss 클래스"],"metadata":{"id":"qVJeDMXjl8PY"}},{"cell_type":"code","source":["class Loss(object):\n","  \"\"\"\n","  신경망 모델의 손실을 게산하는 클래스\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"Pass\"\"\"\n","    pass\n","\n","  def forward(self, prediction: ndarray, target: ndarray) -> float:\n","    \"\"\"\n","    실제 손실값을 계산함\n","    \"\"\"\n","    assert_same_shape(prediction, target)\n","\n","    self.prediction = prediction\n","    self.target = target\n","\n","    loss_value = self._output()\n","\n","    return loss_value\n","\n","  def backward(self) -> ndarray:\n","    \"\"\"\n","    손실함수의 입력값에 대해 손실의 기울기를 계산함\n","    \"\"\"\n","    self.input_grad = self._input_grad()\n","\n","    assert_same_shape(self.prediction, self.input_grad)\n","\n","    return self.input_grad\n","\n","  def _output(self) -> float:\n","    \"\"\"\n","    Loss 클래스를 확장한 모든 구상 클래스는 이 메서드를 구현해야 함\n","    \"\"\"\n","    raise NotImplementedError()\n","\n","  def _input_grad(self) -> ndarray:\n","    \"\"\"\n","    Loss 클래스를 확장한 모든 구상 클래스는 이 메서드를 구현해야 함\n","    \"\"\"\n","    raise NotImplementedError()"],"metadata":{"id":"kS1Q2I4HlGVu","executionInfo":{"status":"ok","timestamp":1642504819764,"user_tz":-540,"elapsed":6,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["MeanSquaredError 클래스"],"metadata":{"id":"5J8fekvZm3Ml"}},{"cell_type":"code","source":["class MeanSquaredError(Loss):\n","\n","  def __init__(self):\n","    \"\"\"Pass\"\"\"\n","    super().__init__()\n","\n","  def _output(self) -> float:\n","    \"\"\"\n","    관찰 단위로 오차를 집계한 평균제곱오차 손실함수\n","    \"\"\"\n","    loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n","\n","    return loss\n","\n","  def _input_grad(self) -> ndarray:\n","    \"\"\"\n","    예측값에 대한 평균제곱오차 손실의 기울기를 계싼\n","    \"\"\"\n","    \n","    return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"],"metadata":{"id":"eKezcUW1m2iG","executionInfo":{"status":"ok","timestamp":1642504819765,"user_tz":-540,"elapsed":7,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## 딥러닝 구현하기\n","\n","1. NeuralNetwork 클래스 객체는 Layer들의 리스트를 속성으로 가지고 있어야 한다. Layer들의 객체는 앞서 정의한 대로 forward와 backward 메서드를 갖추고 있으며 이들 메서드는 ndarray 객체를 인자로 받고 반환한다.\n","2. 각각의 Layer 클래스 객체는 Operation 객체의 리스트를 operations 속성에 담고 있으며 _setup_layer 메서드에 이 리스트가 저장된다.\n","3. Operation 클래스 객체 역시 Layer와 마찬가지로 forward와 backward, 이 2개의 메서드를 갖추고 있으며 이 메서드 역시 ndarray 객체를 인자로 받고 반환한다.\n","4. 각 Operation 객체의 backward 메서드에서 인자로 받은 output_grad는 Layer의 output 속성과 모양이 같아야 한다. input\\_grad와 input_ 속성 역시 모양이 같아야 한다.\n","5. 연산 중에 파라미터(param 속성에 저장된다)가 있는 연산이 있다. 이 연산은 ParamOpertaion 클래스를 확장해 정의한다. 또한 Layer의 forward 와 backward 메서드에서 인자로 받은 ndarray의 모양에 대한 제약(input과 output이 각각 그에 해당하는 기울기와 모양이 일치해야 함)이 여기서도 마찬가지로 적용된다.\n","6. NeuralNetwork 클래스는 Loss 클래스로 정의되는 손실함수를 가져야 한다. 이 클래스는 NeuralNetwork 클래스 객체의 마지막 연산의 출력값과 목푯값을 인자로 받아 이들의 모양이 같은지 검사한다음, 이들로부터 손실(단일한 숫자값)과 손실의 기울기(ndarray 객체)를 계산한다. 이 값이 다시 출력층으로 되돌아가면서 역방향 계산이 시작된다."],"metadata":{"id":"U6uYWw0GqQJQ"}},{"cell_type":"markdown","source":["#### 배치 학습 구현하기\n","1. 데이터를 모델에입력해 함수를 통과시켜(순방향 계산) 예측값을 구한다.\n","2. 손실값을 계산한다.\n","3. 연쇄법칙과 순방향 계산 과정에서 계산된 값을 이용해 파라미터에 대한 손실값의 기울기를 계산한다.\n","4. 기울기를 이용해 파라미터를 수정한다.\n","\n","그리고 새로운 데이터 배치를 가져와 위 과정을 반복한다.\n","\n","<br/>\n","\n","위의 절차를 그대로 코드로 옮겨 NeuralNetwork 프레임워크에 이 절차를 도입한다.\n","1. X와 y를 입력받는다. 이 두 값은 모두 ndarray 객체다.\n","2. X를 각 Layer에 순서대로 통과시키며 순방향 계산을 수행한다.\n","3. Loss 클래스에서 손실값과 손실값의 기울기를 계산한다.\n","4. backward 메서드에서 손실값의 기울기를 전달받아 역방향 계산을 수행한다. 이 과정에서 각 층마다 param_grads를 계산한다.\n","5. 각 층마다 update_params 메서드를 호출해서 NeuralNetwork에 정의된 전체 학습률에 맞춰 param_grads의 방향으로 파라미터를 수정한다."],"metadata":{"id":"9lqM7kndsCmo"}},{"cell_type":"markdown","source":["NeuralNetwork 클래스"],"metadata":{"id":"o0-mbn9hsSVf"}},{"cell_type":"code","source":["class NeuralNetwork(object):\n","  \"\"\"\n","  신경망을 나타내는 클래스\n","  \"\"\"\n","  def __init__(self, layers: List[Layer],\n","               loss: Loss,\n","               seed: float = 1):\n","    \"\"\"\n","    신경망의 층과 손실함수를 정의\n","    \"\"\"\n","    self.layers = layers\n","    self.loss = loss\n","    self.seed = seed\n","    if seed:\n","      for layer in self.layers:\n","        setattr(layer, 'seed', self.seed)    # object에 존재하는 속성의 값을 바꾸거나, 새로운 속성을 생성하여 값을 부여\n","\n","  def forward(self, x_batch: ndarray) -> ndarray:\n","    \"\"\"\n","    데이터를 각 층에 순서대로 통과시킴(순방향 계산)\n","    \"\"\"\n","    x_out = x_batch\n","    for layer in self.layers:\n","      x_out = layer.forward(x_out)\n","\n","    return x_out\n","\n","  def backward(self, loss_grad: ndarray) -> None:\n","    \"\"\"\n","    데이터를 각 층에 역순으로 통과시킴(역방향 계산)\n","    \"\"\"\n","\n","    grad = loss_grad\n","    for layer in reversed(self.layers):\n","      grad = layer.backward(grad)\n","\n","    return None\n","\n","  def train_batch(self,\n","                  x_batch: ndarray,\n","                  y_batch: ndarray) -> float:\n","    \"\"\"\n","    순방향 계산 수행\n","    손실값 계산\n","    역방향 계산 수행\n","    \"\"\"\n","\n","    predictions = self.forward(x_batch)\n","    \n","    loss = self.loss.forward(predictions, y_batch)\n","\n","    self.backward(self.loss.backward())\n","\n","    return loss\n","\n","  def params(self):\n","    \"\"\"\n","    신경망의 파라미터값을 받음\n","    \"\"\"\n","    for layer in self.layers:\n","      yield from layer.params     # 값 함수 밖으로 하나씩 전달\n","\n","  def param_grads(self):\n","    \"\"\"\n","    신경망의 각 파라미터에 대한 손실값의 기울기를 받음\n","    \"\"\"\n","    for layer in self.layers:\n","      yield from layer.param_grads"],"metadata":{"id":"y-wuyjrwnezO","executionInfo":{"status":"ok","timestamp":1642504819765,"user_tz":-540,"elapsed":7,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Optimizer와 Trainer 클래스"],"metadata":{"id":"kRniPIfKwKFy"}},{"cell_type":"markdown","source":["Optimizer 클래스"],"metadata":{"id":"JZAU_FbDxLxI"}},{"cell_type":"code","source":["class Optimizer(object):\n","  \"\"\"\n","  신경망을 최적화하는 기능을 제공하는 추상 클래스\n","  \"\"\"\n","  def __init__(self, \n","               lr: float = 0.01):\n","    \"\"\"\n","    초기 학습률이 반드시 설정되어야 한다.\n","    \"\"\"\n","    self.lr = lr\n","\n","  def step(self) -> None:\n","    \"\"\"\n","    Optimizer를 구현하는 구상 클래스는 이 메서드를 구현해야 한다.\n","    \"\"\"\n","    pass"],"metadata":{"id":"DCdNuahYwYeT","executionInfo":{"status":"ok","timestamp":1642504819765,"user_tz":-540,"elapsed":7,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["SGD 클래스"],"metadata":{"id":"LrNsIQROxijF"}},{"cell_type":"code","source":["class SGD(Optimizer):\n","  \"\"\"\n","  확률적 경사 하강법을 적용한 Optimizer\n","  \"\"\"\n","  def __init__(self,\n","               lr: float = 0.01) -> None:\n","    \"\"\" Pass \"\"\"\n","    super().__init__()\n","\n","  def step(self):\n","    \"\"\"\n","    각 파라미터에 학습률을 곱해 기울기 방향으로 수정함\n","    \"\"\"\n","    for (param, param_grad) in zip(self.net.params(),\n","                                   self.net.param_grads()):\n","      param -= self.lr * param_grad"],"metadata":{"id":"kk72ahIBxgzM","executionInfo":{"status":"ok","timestamp":1642504819765,"user_tz":-540,"elapsed":7,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Trainer 클래스"],"metadata":{"id":"u2o33LL9yAMx"}},{"cell_type":"code","source":["from copy import deepcopy\n","\n","class Trainer(object):\n","  \"\"\"\n","  신경망 모델을 학습시키는 역할을 수행함\n","  \"\"\"\n","\n","  def __init__(self,\n","               net: NeuralNetwork,\n","               optim: Optimizer) -> None:\n","    \"\"\"\n","    학습을 수행하려면 NeuralNetwork, Optimizer 객체가 필요함\n","    Optimizer 객체의 인스턴스 변수로 NeuralNetwork 객체를 전달할 것\n","    \"\"\"\n","    self.net = net\n","    self.optim = optim\n","    self.best_loss = 1e9\n","    setattr(self.optim, 'net', self.net)\n","\n","  def generate_batches(self,\n","                       X: ndarray,\n","                       y: ndarray,\n","                       size: int = 32) -> Tuple[ndarray]:\n","    \"\"\"\n","    배치 생성\n","    \"\"\"\n","    assert X.shape[0] == y.shape[0], \\\n","    f\"\"\"\n","    특징과 목표값은 행의 수가 같아야 하는,\n","    특징은 {X.shape[0]}행, 목표값은 {y.shape[0]}행이다.\n","    \"\"\"\n","    N = X.shape[0]\n","\n","    for ii in range(0, N, size):\n","      x_batch, y_batch = X[ii : ii * size], y[ii : ii * size]\n","\n","      yield x_batch, y_batch\n","\n","  def fit(self, X_train: ndarray, y_train: ndarray,\n","          X_test: ndarray, y_test: ndarray,\n","          epochs: int = 100,\n","          eval_every: int = 10,\n","          batch_size: int = 32,\n","          seed: int = 1,\n","          restart: bool = True) -> None:\n","    \"\"\"\n","    일정 횟수의 에폭을 수행하여 학습 데이터에 신경망을 최적화함\n","    eval_every 변수에 설정된 횟수의 매 에폭마다 테스트 데이터로\n","    신경망의 에측 성능을 측정함\n","    \"\"\"\n","\n","    np.random.seed(seed)\n","\n","    if restart:\n","      for layer in self.net.layers:\n","        layer.first = True\n","\n","      self.best_loss = 1e9\n","\n","    for e in range(epochs):\n","\n","      if (e + 1) % eval_every == 0:\n","        # 조기종료\n","        last_model = deepcopy(self.net)\n","\n","      X_train, y_train = permute_data(X_train, y_train)\n","      batch_generator = self.generate_batches(X_train, y_train, batch_size)\n","\n","      for ii, (X_batch, y_batch) in enumerate(batch_generator):\n","        self.net.train_batch(X_batch, y_batch)\n","        self.optim.step()\n","\n","      if (e + 1) % eval_every == 0:\n","        test_preds = self.net.forward(X_test)\n","        loss = self.net.loss.forward(test_preds, y_test)\n","\n","        if loss < self.best_loss:\n","          print(f'{e+1} 에폭에서 검증 데이터에 대한 손실값: {loss:.3f}')\n","          self.best_loss = loss\n","\n","        else:\n","          print(f'''{e+1} 에폭에서 손실 값이 증가했다. \n","          마지막으로 측정한 손실값은 {e+1-eval_every} 에폭까지 학습된 모델에서 계산된 {self.best_loss:.3f}이다.''')\n","          self.net = last_model\n","          # self.optim이 self.net을 수정하도록 다시 설정\n","          setattr(self.optim, 'net', self.net)\n","          break"],"metadata":{"id":"6o44FTYBx_Vr","executionInfo":{"status":"ok","timestamp":1642505048198,"user_tz":-540,"elapsed":260,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["평가 기준"],"metadata":{"id":"yeUvvxAs2Lyh"}},{"cell_type":"code","source":["def mae(y_true: ndarray, y_pred: ndarray):\n","  \"\"\"\n","  신경망 모델의 평균절대오차 계산\n","  \"\"\"\n","  return np.mean(np.abs(y_true - y_pred))\n","\n","def rmse(y_true: ndarray, y_pred: ndarray):\n","  \"\"\"\n","  신경망 모델의 제곱근 평균제곱오차 계산\n","  \"\"\"\n","  return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n","\n","def eval_regression_model(model: NeuralNetwork,\n","                          X_test: ndarray,\n","                          y_test: ndarray):\n","  \"\"\"\n","  신경망 모델의 평균절대오차 및 제곱근 평균제곱오차 계산\n","  Compute mae and rmse for a neural network.\n","  \"\"\"\n","  preds = model.forward(X_test)\n","  preds = preds.reshape(-1, 1)\n","  print(f'평균절대오치: {mae(preds, y_test):.2f}')\n","  print()\n","  print(f'제곱근 평균제곱오차: {rmse(preds, y_test):.2f}')"],"metadata":{"id":"1mYltms-15Kg","executionInfo":{"status":"ok","timestamp":1642504928502,"user_tz":-540,"elapsed":386,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["lr = NeuralNetwork(\n","    layers=[Dense(neurons=1,\n","                   activation=Linear())],\n","    loss=MeanSquaredError(),\n","    seed=20190501\n",")\n","\n","nn = NeuralNetwork(\n","    layers=[Dense(neurons=13,\n","                   activation=Sigmoid()),\n","            Dense(neurons=1,\n","                   activation=Linear())],\n","    loss=MeanSquaredError(),\n","    seed=20190501\n",")\n","\n","dl = NeuralNetwork(\n","    layers=[Dense(neurons=13,\n","                   activation=Sigmoid()),\n","            Dense(neurons=13,\n","                   activation=Sigmoid()),\n","            Dense(neurons=1,\n","                   activation=Linear())],\n","    loss=MeanSquaredError(),\n","    seed=20190501\n",")"],"metadata":{"id":"NYsFvgdD2_Ht","executionInfo":{"status":"ok","timestamp":1642504820087,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["데이터 로드, 테스트 / 학습 데이터 분할"],"metadata":{"id":"3xDIJaW33HRS"}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\n","\n","boston = load_boston()\n","data = boston.data\n","target = boston.target\n","features = boston.feature_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7lWVCX03GNe","executionInfo":{"status":"ok","timestamp":1642504820087,"user_tz":-540,"elapsed":4,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}},"outputId":"27f8c490-8934-4ced-c8d3-b5a5f43abe1a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","    \n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["# 데이터 축척 변환\n","\n","from sklearn.preprocessing import StandardScaler\n","s = StandardScaler()\n","data = s.fit_transform(data)"],"metadata":{"id":"oZQz-xpw3Sb-","executionInfo":{"status":"ok","timestamp":1642504820088,"user_tz":-540,"elapsed":4,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def to_2d_np(a: ndarray, \n","          type: str=\"col\") -> ndarray:\n","    '''\n","    1차원 텐서를 2차원으로 변환\n","    '''\n","\n","    assert a.ndim == 1, \"입력된 텐서는 1차원이어야 함\"\n","    \n","    if type == \"col\":        \n","        return a.reshape(-1, 1)\n","    elif type == \"row\":\n","        return a.reshape(1, -1)"],"metadata":{"id":"EDPHEr1K3ZYu","executionInfo":{"status":"ok","timestamp":1642504820088,"user_tz":-540,"elapsed":4,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data, target, test_size = 0.3, random_state = 80718)\n","\n","# 목표값을 2차원 배열로 변환\n","y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"],"metadata":{"id":"VJ0DEhVi3kyt","executionInfo":{"status":"ok","timestamp":1642504820387,"user_tz":-540,"elapsed":303,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["3가지 모델 학습"],"metadata":{"id":"-G5z52gd33DC"}},{"cell_type":"code","source":["# 헬퍼 함수\n","\n","def permute_data(X, y):\n","  perm = np.random.permutation(X.shape[0])\n","  return X[perm], y[perm]"],"metadata":{"id":"bVqR_Mqs32Q_","executionInfo":{"status":"ok","timestamp":1642504820388,"user_tz":-540,"elapsed":5,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(lr, SGD(lr = 0.01))\n","\n","trainer.fit(X_train, y_train, X_test, y_test,\n","            epochs = 50,\n","            eval_every = 10,\n","            seed = 20190501)\n","\n","print()\n","eval_regression_model(lr, X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCZ76PVS4DAV","executionInfo":{"status":"ok","timestamp":1642505051551,"user_tz":-540,"elapsed":265,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}},"outputId":"5693d0c9-68f1-4948-ba60-5df032da5e03"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["10 에폭에서 검증 데이터에 대한 손실값: 32.721\n","20 에폭에서 검증 데이터에 대한 손실값: 28.873\n","30 에폭에서 검증 데이터에 대한 손실값: 26.134\n","40 에폭에서 검증 데이터에 대한 손실값: 25.631\n","50 에폭에서 검증 데이터에 대한 손실값: 25.155\n","\n","평균절대오치: 3.53\n","\n","제곱근 평균제곱오차: 5.02\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"code","source":["trainer = Trainer(nn, SGD(lr = 0.01))\n","\n","trainer.fit(X_train, y_train, X_test, y_test,\n","            epochs = 50,\n","            eval_every = 10,\n","            seed = 20190501)\n","\n","print()\n","eval_regression_model(nn, X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mleoShK87kZj","executionInfo":{"status":"ok","timestamp":1642505111921,"user_tz":-540,"elapsed":267,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}},"outputId":"c76d277e-9722-4339-be5f-66fb080ba257"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["10 에폭에서 검증 데이터에 대한 손실값: 29.225\n","20 에폭에서 검증 데이터에 대한 손실값: 22.751\n","30 에폭에서 검증 데이터에 대한 손실값: 19.623\n","40 에폭에서 검증 데이터에 대한 손실값: 17.861\n","50 에폭에서 검증 데이터에 대한 손실값: 16.730\n","\n","평균절대오치: 2.65\n","\n","제곱근 평균제곱오차: 4.09\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"code","source":["trainer = Trainer(dl, SGD(lr=0.01))\n","\n","trainer.fit(X_train, y_train, X_test, y_test,\n","       epochs = 50,\n","       eval_every = 10,\n","       seed=20190501);\n","print()\n","eval_regression_model(dl, X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcQCSfZo7wYf","executionInfo":{"status":"ok","timestamp":1642505121472,"user_tz":-540,"elapsed":678,"user":{"displayName":"임희진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_oO81oyEPaQrdiS5Ku5ChIDaiAYe3xkkp5CCLOw=s64","userId":"01957954719942143952"}},"outputId":"241351ca-233f-43d6-9abd-afac733da4c9"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["10 에폭에서 검증 데이터에 대한 손실값: 48.183\n","20 에폭에서 검증 데이터에 대한 손실값: 26.772\n","30 에폭에서 검증 데이터에 대한 손실값: 22.721\n","40 에폭에서 검증 데이터에 대한 손실값: 17.378\n","50 에폭에서 검증 데이터에 대한 손실값: 15.922\n","\n","평균절대오치: 2.61\n","\n","제곱근 평균제곱오차: 3.99\n"]}]}]}